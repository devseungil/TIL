{
 "cells": [
  {
   "cell_type": "raw",
   "id": "6e26410f",
   "metadata": {},
   "source": [
    "<<LSTM 모형>>\n",
    "1 ) Embedding : 복수의 문장을 입력했을때 문장의 단어를 벡터표현으로 변환하고 리턴해줌\n",
    "\n",
    "2 ) GlobalAveragePooling1D : Embedding에서 문장을 벡터로 리턴하면 단어 벡터의 차원별로 평균을 리턴\n",
    "                             각 문장의 특징을 압축한 데이터로 표현\n",
    "\n",
    "tf.keras.layers.Embedding(\n",
    "    input_dim,\n",
    "    output_dim,\n",
    "    embeddings_initializer='uniform',\n",
    "    embeddings_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    embeddings_constraint=None,\n",
    "    mask_zero=False,\n",
    "    input_length=None,\n",
    "    **kwargs )\n",
    "\n",
    "tf.keras.layers.GlobalAveragePooling1D(\n",
    "    data_format='channels_last', **kwargs )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "953fed14",
   "metadata": {},
   "source": [
    "LSTM : 텍스트 생성 모델 적용, 순환신경망(RNN)의 한 종류 / GAN : 이미지 생성모델 적용\n",
    "    = 단어의 시퀀스가 주어지면 다음 단어를 예측하도록 훈련하는 모델\n",
    "    - Embedding : 토큰화된 정수를 인풋사이즈의 벡터로 변환\n",
    "                  학습되는 가중치의 개수 = 어휘사전의 크기 * embedding_size\n",
    "                                              output_shape     param (토큰4169 * 임베딩100)\n",
    "                  ex) embedding_1(Embedding) (None, None, 100) 416900\n",
    "\n",
    "1. 텍스트 데이터는 개별적인 데이터 조각으로 구성 즉 문자나 단어로 구성된것\n",
    "    시간차원(단어의 순서, 조합)은 있지만 공간차원은없다\n",
    "    문자나 단어 변화에 민감하다\n",
    "    텍스트는 규칙기반(문법)\n",
    "    \n",
    "2. 이미지는 공간차원은 있지만 시간차원은 없다 , 문법이 없다\n",
    "\n",
    "<<토큰화>>\n",
    "1) 단어 토큰\n",
    "    모든 텍스트를 소문자로 변환해야함(단, 지명 또는 고유명사, 이름 등은 제외)\n",
    "    어간추출(시제가 다른 동사들을 하나로 토큰화), 구두점(마침표,쉼표등)을 토큰화 하거나 제거\n",
    "    [주의] 단어 토큰화를 사용하면 train 어휘에 없는 단어는 예측 불가능\n",
    "    \n",
    "2) 문자 토큰\n",
    "    모델이 문자의 시퀀스를 생성하서 train어휘에 없는 새로운 단어를 생성할수있음"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2dcccc75",
   "metadata": {},
   "source": [
    "Q1) 문자열을 벡터화 시켜보자\n",
    "Hope :0\n",
    "to   :1\n",
    "see  :2\n",
    "you  :3\n",
    "soon :4\n",
    "Nice :5\n",
    "again:6\n",
    "============================Embedding\n",
    "문장화 --> Hope to see you soon \n",
    "            0   1  2   3    4\n",
    "          [ 0.02843983 -0.04518877] Hope\n",
    "          [ 0.01244213  0.02208966]  to\n",
    "          [ 0.03567721  0.04178825]  see\n",
    "          [ 0.04183492 -0.03163595]  you\n",
    "          [-0.02534178 -0.01245432]  soon\n",
    "문장화 --> Nice to see you again\n",
    "            5   1   2  3    6\n",
    "          [ 0.00433122 -0.04353247] Nice\n",
    "          [ 0.01244213  0.02208966]  to\n",
    "          [ 0.03567721  0.04178825]  see\n",
    "          [ 0.04183492 -0.03163595]  you\n",
    "          [ 0.04732199  0.00539281]  again\n",
    "========================= GlobalAveragePooling1D\n",
    "[[ 0.01861046 -0.00508023]  = > Hope to see you soon\n",
    " [ 0.0283215  -0.00117954]] = > Nice to see you again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "04454823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization , Embedding , GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4568de4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n",
      "[[[ 0.02843983 -0.04518877]\n",
      "  [ 0.01244213  0.02208966]\n",
      "  [ 0.03567721  0.04178825]\n",
      "  [ 0.04183492 -0.03163595]\n",
      "  [-0.02534178 -0.01245432]]\n",
      "\n",
      " [[ 0.00433122 -0.04353247]\n",
      "  [ 0.01244213  0.02208966]\n",
      "  [ 0.03567721  0.04178825]\n",
      "  [ 0.04183492 -0.03163595]\n",
      "  [ 0.04732199  0.00539281]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 5, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 문자열을 벡터를 이용한 모델을 생성해서 값을 리턴받아보자\n",
    "# 2-1 문장 데이터를 변수에 저장\n",
    "input_array = np.array([[0,1,2,3,4],[5,1,2,3,6]])\n",
    "\n",
    "# 2-2 랜덤 속성값 설정 (난수 똑같이나오게 지정)\n",
    "np.random.seed(1)\n",
    "tf.keras.utils.set_random_seed(2)\n",
    "\n",
    "# 2-3 모델 생성하기\n",
    "model = Sequential()\n",
    "model.add(Embedding(7,2))\n",
    "# model.add(GlobalAveragePooling1D()) # 평균리턴\n",
    "model.add(LSTM(3, activation='tanh'))\n",
    "\n",
    "model.compile('rmsprop' , 'mse')\n",
    "model.fit(input_array)\n",
    "\n",
    "output_array = model.predict(input_array)\n",
    "print(output_array)\n",
    "print(output_array.shape) # Embedding만하면 (2,5,2)\n",
    "                          # GlobalAveragePooling1D하면 (2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb2c401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 문자열을 벡터를 이용한 모델을 생성해서 값을 리턴받아보자\n",
    "# 2-1 문장 데이터를 변수에 저장\n",
    "input_array = np.array([[0,1,2,3,4],[5,1,2,3,6]])\n",
    "# 2-2 랜덤 속성값 설정 (난수 똑같이나오게 지정)\n",
    "np.random.seed(1)\n",
    "tf.keras.utils.set_random_seed(2)\n",
    "\n",
    "# 2-3 모델 생성하기\n",
    "model = Sequential()\n",
    "model.add(Embedding(7,2))\n",
    "# model.add(GlobalAveragePooling1D()) # 평균리턴\n",
    "model.compile('rmsprop' , 'mse')\n",
    "\n",
    "output_array = model.predict(input_array)\n",
    "print(output_array)\n",
    "print(output_array.shape) # Embedding만하면 (2,5,2)\n",
    "                          # GlobalAveragePooling1D하면 (2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74900aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 [==============================] - 2s 2s/step - loss: 49.2413\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 49.2074\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 49.1823\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 49.1607\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 49.1410\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 49.1226\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 49.1050\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 49.0881\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 49.0717\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 49.0556\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 49.0397\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 49.0241\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 49.0086\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 48.9931\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 48.9778\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 48.9625\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 48.9472\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 48.9319\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 48.9165\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 48.9011\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 48.8857\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 48.8702\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 48.8547\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 48.8391\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 48.8234\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 48.8076\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 48.7917\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 48.7757\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 48.7596\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 48.7434\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 48.7270\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 48.7106\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 48.6940\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 48.6774\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 48.6606\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 48.6436\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 48.6266\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 48.6094\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 48.5921\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 48.5746\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 48.5570\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 48.5393\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 48.5215\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 48.5035\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 48.4854\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 48.4671\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 48.4487\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 48.4302\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 48.4115\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 48.3927\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001B30BCBD4C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 293ms/step\n",
      "[[0.5625312]\n",
      " [0.5631644]]\n",
      "(2, 1)\n"
     ]
    }
   ],
   "source": [
    "# 2 문자열을 벡터를 이용한 모델을 생성해서 값을 리턴받아보자\n",
    "# 2-1 문장 데이터를 변수에 저장\n",
    "input_array = np.array([[0,1,2,3,4],[5,1,2,3,6]]) # 텍스트를 토큰화\n",
    "x_train = input_array.reshape(input_array.shape[0],input_array.shape[1],1)\n",
    "y_train = np.array([7,8])\n",
    "# 2-2 랜덤 속성값 설정 (난수 똑같이나오게 지정)\n",
    "np.random.seed(1)\n",
    "tf.keras.utils.set_random_seed(2)\n",
    "\n",
    "# 2-3 모델 생성하기 : 출력은 시퀀스 다음에 어휘사전에 등장할 수 있는 단어의 확률\n",
    "model = Sequential()\n",
    "model.add(Embedding(7,2))\n",
    "# model.add(GlobalAveragePooling1D()) # 평균리턴\n",
    "model.add(LSTM(3, activation='tanh'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile('rmsprop' , 'mse')\n",
    "model.fit(x_train,y_train , batch_size=3, epochs=50, verbose=1)\n",
    "\n",
    "output_array = model.predict(input_array)\n",
    "print(output_array)\n",
    "print(output_array.shape) # Embedding만하면 (2,5,2)\n",
    "                          # GlobalAveragePooling1D하면 (2,2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fbda3152",
   "metadata": {},
   "source": [
    "#5 LSTM 적용해보자 [batch , 3 , 2]\n",
    "LSTM input = [batch , timesteps , feature] 밑에건 timesteps = 3 으로했음 밑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "35bbe47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a40bd72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./my_data.csv')\n",
    "no_of_lag = 3\n",
    "\n",
    "#5-2 특징량 작성을 위한 쉬프트 함수를 만들어 결합\n",
    "df_lag_X = pd.DataFrame()\n",
    "for i in range(1,no_of_lag):\n",
    "    df_lag_X['x_rag{}'.format(i)] = df['기온'].shift(i)\n",
    "    \n",
    "df_lag_y = pd.DataFrame()\n",
    "for i in range(1,no_of_lag):\n",
    "    df_lag_y['y_rag{}'.format(i)] = df['습도'].shift(i)\n",
    "    \n",
    "# 조인\n",
    "df_temp = pd.concat([df['기온'] ,df_lag_X ] , axis=1)\n",
    "df_humi = pd.concat([df['습도'] ,df_lag_y ] , axis=1)\n",
    "\n",
    "df_temp.dropna(inplace=True)\n",
    "df_humi.dropna(inplace=True)\n",
    "\n",
    "# reshape로 일렬로 처리\n",
    "temp = np.array(df_temp).reshape(-1,no_of_lag)\n",
    "humi = np.array(df_humi).reshape(-1,no_of_lag)\n",
    "\n",
    "# [batch , timesteps , feature]\n",
    "input_format = []\n",
    "_format1 = []\n",
    "for j in range(0,len(temp)):\n",
    "    for i in range(0,no_of_lag):\n",
    "        _m = ( [temp[j][i] , humi[j][i]] )\n",
    "        _format1.append(_m)\n",
    "    input_format.append(_format1)\n",
    "    _format1 = []\n",
    "\n",
    "input_format = np.array(input_format)\n",
    "y = pd.concat([df['날씨']],axis=1)\n",
    "y = y[no_of_lag-1: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "becf5da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 3, 2)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_format.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7d15cc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x , test_x , train_y , test_y = train_test_split(input_format , y)\n",
    "cb_e = tf.keras.callbacks.EarlyStopping(monitor='val_loss')\n",
    "cb_save = tf.keras.callbacks.ModelCheckpoint(filepath='mycheck' , monitor='val_loss' , save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6e9c42c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.array(train_y)\n",
    "train_x = np.array(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "af5c845b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/1 [==============================] - ETA: 0s - loss: 10.7642 - acc: 0.2500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, lstm_cell_6_layer_call_fn, lstm_cell_6_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mycheck\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mycheck\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 5s 5s/step - loss: 10.7642 - acc: 0.2500 - val_loss: 2.6479 - val_acc: 0.0000e+00\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - ETA: 0s - loss: 7.7998 - acc: 0.2500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, lstm_cell_6_layer_call_fn, lstm_cell_6_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mycheck\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mycheck\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 3s 3s/step - loss: 7.7998 - acc: 0.2500 - val_loss: 1.7162 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b3175f9a60>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5-9 모델작성\n",
    "#LSTM 모델 작성 주의점 : 데이터의 순서가 새로운 시간의 데이터로 시작되기 때문에 (t , t-1 , t-2)\n",
    "\n",
    "input01 = Input(shape = (train_x.shape[1] , train_x.shape[2]))\n",
    "lstm = LSTM(16,go_backwards=True)(input01)\n",
    "drop01 = Dropout(0.2)(lstm)\n",
    "d1 = Dense(10)(drop01)\n",
    "outputs = Dense(3)(d1)\n",
    "\n",
    "model = Model(inputs = input01 , outputs = outputs)\n",
    "model.compile('adam' , 'binary_crossentropy' , ['acc'])\n",
    "model.fit(train_x , train_y , epochs = 5 , batch_size = 5 , validation_split=0.1, callbacks=[cb_e, cb_save])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "02306209",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001B31BEBF820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001B31BEBF820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 353ms/step\n",
      "[[ 0.49463302 -0.03723762 -0.05681894]\n",
      " [ 0.48544335 -0.06318074 -0.1621387 ]]\n"
     ]
    }
   ],
   "source": [
    "pred_y = model.predict(test_x)\n",
    "print(pred_y)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "df8c55a5",
   "metadata": {},
   "source": [
    "LSTM input = 3D tensor shape [batch , timesteps 3 = (t , t-1, t-2) , feature]\n",
    "                             => 샘플수 , look_back ,                  변수 수\n",
    "                             timesteps = 과거 몇 회분의 데이터를 1개의 데이터로 간주할까 3이면 3주의데이터를 1개의데이터로 간주"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6333f41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f36d947b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('./city_weather.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0802eb26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ave_temperature</th>\n",
       "      <th>ave_humidity</th>\n",
       "      <th>total_daylight_hours</th>\n",
       "      <th>total_precipitation</th>\n",
       "      <th>ave_wind_speed</th>\n",
       "      <th>ave_cloud_cover</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-12-21</th>\n",
       "      <td>9.0</td>\n",
       "      <td>43</td>\n",
       "      <td>49.3</td>\n",
       "      <td>43.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-12-28</th>\n",
       "      <td>6.5</td>\n",
       "      <td>43</td>\n",
       "      <td>48.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-04</th>\n",
       "      <td>5.6</td>\n",
       "      <td>35</td>\n",
       "      <td>59.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-11</th>\n",
       "      <td>4.0</td>\n",
       "      <td>36</td>\n",
       "      <td>50.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-18</th>\n",
       "      <td>5.6</td>\n",
       "      <td>40</td>\n",
       "      <td>56.6</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-17</th>\n",
       "      <td>17.0</td>\n",
       "      <td>65</td>\n",
       "      <td>46.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-24</th>\n",
       "      <td>11.2</td>\n",
       "      <td>70</td>\n",
       "      <td>26.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>7.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-01</th>\n",
       "      <td>8.6</td>\n",
       "      <td>77</td>\n",
       "      <td>30.5</td>\n",
       "      <td>11.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-08</th>\n",
       "      <td>10.1</td>\n",
       "      <td>68</td>\n",
       "      <td>25.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>7.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-15</th>\n",
       "      <td>5.2</td>\n",
       "      <td>45</td>\n",
       "      <td>50.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>522 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ave_temperature  ave_humidity  total_daylight_hours  \\\n",
       "date                                                              \n",
       "2010-12-21              9.0            43                  49.3   \n",
       "2010-12-28              6.5            43                  48.7   \n",
       "2011-01-04              5.6            35                  59.2   \n",
       "2011-01-11              4.0            36                  50.8   \n",
       "2011-01-18              5.6            40                  56.6   \n",
       "...                     ...           ...                   ...   \n",
       "2020-11-17             17.0            65                  46.9   \n",
       "2020-11-24             11.2            70                  26.0   \n",
       "2020-12-01              8.6            77                  30.5   \n",
       "2020-12-08             10.1            68                  25.2   \n",
       "2020-12-15              5.2            45                  50.7   \n",
       "\n",
       "            total_precipitation  ave_wind_speed  ave_cloud_cover  \n",
       "date                                                              \n",
       "2010-12-21                 43.5             3.0              2.6  \n",
       "2010-12-28                  0.0             2.4              4.1  \n",
       "2011-01-04                  0.0             3.6              2.1  \n",
       "2011-01-11                  0.0             2.7              3.8  \n",
       "2011-01-18                  3.5             2.4              3.8  \n",
       "...                         ...             ...              ...  \n",
       "2020-11-17                  0.0             3.2              5.1  \n",
       "2020-11-24                  5.0             2.3              7.7  \n",
       "2020-12-01                 11.5             2.0              5.6  \n",
       "2020-12-08                  0.0             2.1              7.1  \n",
       "2020-12-15                  0.0             2.3              2.4  \n",
       "\n",
       "[522 rows x 6 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['date'] = pd.to_datetime(df1['date'])\n",
    "df1.set_index('date',inplace=True) # date를 인덱스로 변경\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8635b649",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
